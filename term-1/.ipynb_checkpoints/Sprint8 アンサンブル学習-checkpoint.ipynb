{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nobu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "sns.set(palette=\"bright\")\n",
    "\n",
    "train = pd.read_csv(\"/Users/nobu/Documents/データセット/house-prices-advanced-regression-techniques/train.csv\")\n",
    "\n",
    "X = train.loc[:,['GrLivArea','YearBuilt']].values\n",
    "y = train[\"SalePrice\"].values\n",
    "\n",
    "#訓練用80%と検証用20%に分割\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "#特徴量を計算・標準化(計算は訓練データのみ)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler()\n",
    "X_train_std = std.fit_transform(X_train)\n",
    "X_test_std = std.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装\n",
    "\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。\n",
    "\n",
    "精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。\n",
    "\n",
    "ブレンディングとは\n",
    "ブレンディングとは、N個の多様なモデルを独立して学習させ、推定結果を重み付けした上で足し合わせる方法です。\n",
    "\n",
    "最も単純には平均をとります。多様なモデルとは、以下のような条件を変化させることで作り出すものです。\n",
    "\n",
    "\n",
    "手法（例：線形回帰、SVM、決定木、ニューラルネットワークなど）\n",
    "\n",
    "ハイパーパラメータ（例：SVMのカーネルの種類、重みの初期値など）\n",
    "\n",
    "入力データの前処理の仕方（例：標準化、対数変換、PCAなど）\n",
    "\n",
    "重要なのはそれぞれのモデルが大きく異なることです。\n",
    "\n",
    "\n",
    "回帰問題でのブレンディングは非常に単純であるため、scikit-learnには用意されていません。\n",
    "\n",
    "\n",
    "《補足》\n",
    "分類問題の場合は、多数決を行います。回帰問題に比べると複雑なため、scikit-learnにはVotingClassifierが用意されています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========MSE==============\n",
      "lr：\n",
      "0.2000\n",
      "===========MSE==============\n",
      "clf：\n",
      "0.3097\n",
      "===========MSE==============\n",
      "tree_5：\n",
      "0.1807\n",
      "===========MSE==============\n",
      "tree_8：\n",
      "0.1900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std,y_train)\n",
    "lr_pred = lr.predict(X_test_std)\n",
    "\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_std,y_train)\n",
    "clf_pred = clf.predict(X_test_std)\n",
    "\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=5)\n",
    "tree.fit(X_train_std,y_train)\n",
    "tree_pred = tree.predict(X_test_std)\n",
    "\n",
    "\n",
    "tree_8 = DecisionTreeRegressor(max_depth=8)\n",
    "tree_8.fit(X_train_std,y_train)\n",
    "tree_8_pred = tree_8.predict(X_test_std)\n",
    "\n",
    "\n",
    "print(\"===========MSE==============\")\n",
    "print(\"lr：\\n{:.4f}\".format(mean_squared_error(y_test,lr_pred)*1e-10))\n",
    "print(\"===========MSE==============\")\n",
    "print(\"clf：\\n{:.4f}\".format(mean_squared_error(y_test,clf_pred)*1e-10))\n",
    "print(\"===========MSE==============\")\n",
    "print(\"tree_5：\\n{:.4f}\".format(mean_squared_error(y_test,tree_pred)*1e-10))\n",
    "print(\"===========MSE==============\")\n",
    "print(\"tree_8：\\n{:.4f}\".format(mean_squared_error(y_test,tree_8_pred)*1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========MSE==============\n",
      "(lr+clf)/2:\n",
      "0.2245\n",
      "===========MSE==============\n",
      "(lr+tree)/2:\n",
      "0.1603\n",
      "===========MSE==============\n",
      "(clf+tree)/2:\n",
      "0.1853\n",
      "===========MSE==============\n",
      "(lr+clf+tree)/3:\n",
      "0.1776\n",
      "===========MSE==============\n",
      "(tree_5+tree_8)/2:\n",
      "0.1589\n"
     ]
    }
   ],
   "source": [
    "blend_pred1 = (lr_pred + clf_pred) / 2\n",
    "print(\"===========MSE==============\")\n",
    "print(\"(lr+clf)/2:\\n{:.4f}\".format(mean_squared_error(y_test, blend_pred1)*1e-10))\n",
    "\n",
    "blend_pred2 = (lr_pred + tree_pred) / 2\n",
    "print(\"===========MSE==============\")\n",
    "print(\"(lr+tree)/2:\\n{:.4f}\".format(mean_squared_error(y_test, blend_pred2)*1e-10))\n",
    "\n",
    "blend_pred3 = (clf_pred + tree_pred) / 2\n",
    "print(\"===========MSE==============\")\n",
    "print(\"(clf+tree)/2:\\n{:.4f}\".format(mean_squared_error(y_test, blend_pred3)*1e-10))\n",
    "\n",
    "blend_pred4 = (lr_pred + clf_pred + tree_pred) / 3\n",
    "print(\"===========MSE==============\")\n",
    "print(\"(lr+clf+tree)/3:\\n{:.4f}\".format(mean_squared_error(y_test, blend_pred4)*1e-10))\n",
    "\n",
    "blend_pred5 = (tree_8_pred + tree_pred) / 2\n",
    "print(\"===========MSE==============\")\n",
    "print(\"(tree_5+tree_8)/2:\\n{:.4f}\".format(mean_squared_error(y_test, blend_pred5)*1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡︎\n",
    "* 線形回帰と決定木の平均をとったもの、\n",
    "* 線形回帰・SVM・決定木の平均をとったもの、\n",
    "* 深さ３と５の決定木を平均したものが、\n",
    "\n",
    "単体のモデルよりも性能が良かった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】バギングのスクラッチ実装\n",
    "\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "バギングとは\n",
    "\n",
    "バギングは入力データの選び方を多様化する方法です。\n",
    "\n",
    "学習データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。\n",
    "\n",
    "それらによってモデルをN個学習し、推定結果の平均をとります。\n",
    "\n",
    "ブレンディングと異なり、それぞれの重み付けを変えることはありません。\n",
    "\n",
    "\n",
    "sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation\n",
    "\n",
    "\n",
    "scikit-learnのtrain_test_splitを、shuffleパラメータをTrueにして使うことで、ランダムにデータを分割することができます。\n",
    "\n",
    "これによりブートストラップサンプルが手に入ります。\n",
    "\n",
    "\n",
    "推定結果の平均をとる部分はブースティングと同様の実装になります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_bugging(X,y,num,time):#特徴量（array）、目的変数(array)、抽出する個数、試行回数\n",
    "    \n",
    "    #predの合計を初期化\n",
    "    total_pred = 0\n",
    "    \n",
    "    for i in range(time):\n",
    "    \n",
    "        #shuffleをTrueにして分割\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)\n",
    "\n",
    "        #特徴量を計算・標準化(計算は訓練データのみ)\n",
    "        std = StandardScaler()\n",
    "        X_train_std = std.fit_transform(X_train[:num,:])#全列の0行からnum行まで抽出(shuffle=Trueなので毎回違う)\n",
    "        X_test_std = std.transform(X_test)\n",
    "\n",
    "        #treeで学習＆推定\n",
    "        tree = DecisionTreeRegressor(max_depth=5)\n",
    "        tree.fit(X_train_std,y_train[:num])#num個抽出\n",
    "        tree_pred = tree.predict(X_test_std)\n",
    "        \n",
    "        #推定結果を合計\n",
    "        total_pred+=tree_pred\n",
    "    \n",
    "    #全試行の推定値の平均を算出\n",
    "    result_pred = total_pred/time\n",
    "    \n",
    "    #算出した推定値を用いてmseを算出\n",
    "    mse = mean_squared_error(y_test,result_pred)\n",
    "    \n",
    "    return round(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========MSE==============\n",
      "tree_5：\n",
      "0.1721\n",
      "===========MSE==============\n",
      "bugging：\n",
      "0.1595\n"
     ]
    }
   ],
   "source": [
    "print(\"===========MSE==============\")\n",
    "print(\"tree_5：\\n{:.4f}\".format(mean_squared_error(y_test,tree_pred)*1e-10))\n",
    "print(\"===========MSE==============\")\n",
    "print(\"bugging：\\n{:.4f}\".format(tree_bugging(X,y,700,100)*1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡︎700個抽出、繰り返し回数１００回で深さ５の決定木単体のモデルより性能が良かった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】スタッキングのスクラッチ実装\n",
    "\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。\n",
    "\n",
    "\n",
    "スタッキングとは\n",
    "スタッキングの手順は以下の通りです。最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは $K_0=3, M_0=2$ 程度にします。\n",
    "\n",
    "\n",
    "《学習時》\n",
    "\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "\n",
    "学習データを $K_0$ 個に分割する。\n",
    "分割した内の $(K_0 - 1)$ 個をまとめて学習用データ、残り $1$ 個を推定用データとする組み合わせが $K_0$ 個作れる。\n",
    "あるモデルのインスタンスを $K_0$ 個用意し、異なる学習用データを使い学習する。\n",
    "それぞれの学習済みモデルに対して、使っていない残り $1$ 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）\n",
    "さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、 $M_0$ 個のブレンドデータが得られる。\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "\n",
    "ステージ $n-1$ のブレンドデータを$M_{n-1}$ 次元の特徴量を持つ学習用データと考え、 $K_n$ 個に分割する。以下同様である。\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ $N-1$ の $M_{N-1}$ 個のブレンドデータを$M_{N-1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。\n",
    "\n",
    "《推定時》\n",
    "\n",
    "\n",
    "（ステージ $0$ ）\n",
    "\n",
    "\n",
    "テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。これを $K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ $n$ ）\n",
    "\n",
    "\n",
    "ステージ $n-1$ で得たブレンドテストを $K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを $K_n$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）\n",
    "\n",
    "（ステージ $N$ ）＊最後のステージ\n",
    "\n",
    "\n",
    "ステージ $N-1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3つのモデルを使ったスタッキングにより、MSEを求める関数\n",
    "\n",
    "\n",
    "def model3_stacking(X,y,model1,model2,model3):\n",
    "    \"\"\"\n",
    "    X:feature(sample,columns)\n",
    "    y:target(sample,1)\n",
    "    model1~3:スタッキングに用いるモデル\n",
    "    \"\"\"\n",
    "      \n",
    "    \"\"\"\n",
    "    ベースモデルの学習、メタモデルの学習のためにデータを６つ作る\n",
    "    \"\"\"\n",
    "    #メタモデル学習のための分割(割合はtrain8:test2)\n",
    "    X_train_valid, X_meta_valid, y_train_valid, y_meta_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #分割したvalデータを用いてベースモデル学習のために分割(割合は5:5)\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X_train_valid,y_train_valid,test_size=0.5,random_state=42) \n",
    "    \"\"\"\n",
    "    ベースモデルの学習\n",
    "    \"\"\"\n",
    "    \n",
    "    #ベースモデルを学習させる\n",
    "    model1.fit(X_train,y_train)\n",
    "    model2.fit(X_train,y_train)\n",
    "    model3.fit(X_train,y_train)\n",
    "    \n",
    "    #ベースモデルを学習させたものを使って推定する\n",
    "    model1_pred = model1.predict(X_test)\n",
    "    model2_pred = model2.predict(X_test)\n",
    "    model3_pred = model3.predict(X_test)   \n",
    "   \n",
    "    #ベースモデルを学習させた結果でメタモデルを推定する\n",
    "    val_model1_pred = model1.predict(X_meta_valid)\n",
    "    val_model2_pred = model2.predict(X_meta_valid)\n",
    "    val_model3_pred = model3.predict(X_meta_valid)\n",
    "    \n",
    "    #各モデル単体のMSEを算出\n",
    "    print(\"model1単体のMSE:\\n{:.4f}\".format(mean_squared_error(y_meta_valid,val_model1_pred)*1e-10))\n",
    "    print(\"model2単体のMSE:\\n{:.4f}\".format(mean_squared_error(y_meta_valid,val_model2_pred)*1e-10))\n",
    "    print(\"model3単体のMSE:\\n{:.4f}\".format(mean_squared_error(y_meta_valid,val_model3_pred)*1e-10))\n",
    "    \n",
    "    \"\"\"\n",
    "    ベースモデルの推定結果を使ってメタモデルを学習させる（メタモデルにはベースモデルの学習に使ったデータは教えない）\n",
    "    \"\"\"\n",
    "    #ベースモデルの推定結果をまとめる\n",
    "    base_predictions = np.column_stack((model1_pred,model2_pred,model3_pred))\n",
    "    \n",
    "    #メタモデルの推定結果をまとめる\n",
    "    meta_predictions = np.column_stack((val_model1_pred,val_model2_pred,val_model3_pred))\n",
    "    \n",
    "    #メタモデルの学習で用いるモデルとして線形回帰のインスタンスを作成\n",
    "    meta_model = LinearRegression()\n",
    "    \n",
    "    #メタモデルの推定結果をまとめたもの入力として学習させる\n",
    "    meta_model.fit(base_predictions,y_test)\n",
    "    \n",
    "    #スタッキングした最終的な予測をする\n",
    "    meta_val_pred = meta_model.predict(meta_predictions)\n",
    "    \n",
    "    #最終的なMSEを算出\n",
    "    print(\"stackingを行った結果のMSE:\\n{:.4f}\".format(mean_squared_error(y_meta_valid,meta_val_pred)*1e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.2493\n",
      "model2単体のMSE:\n",
      "0.4856\n",
      "model3単体のMSE:\n",
      "0.2675\n",
      "stackingを行った結果のMSE:\n",
      "0.2333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "LR = LinearRegression()\n",
    "CLF = svm.SVC()\n",
    "TREE = DecisionTreeRegressor()\n",
    "RFR =  RandomForestRegressor()\n",
    "LGB = lgb.LGBMClassifier()\n",
    "\n",
    "\n",
    "model3_stacking(X,y,\n",
    "                model1=LR,\n",
    "                model2 = CLF,\n",
    "                model3 = TREE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.2493\n",
      "model2単体のMSE:\n",
      "0.4856\n",
      "model3単体のMSE:\n",
      "0.1608\n",
      "stackingを行った結果のMSE:\n",
      "0.1978\n"
     ]
    }
   ],
   "source": [
    "model3_stacking(X,y,\n",
    "                model1=LR,\n",
    "                model2 = CLF,\n",
    "                model3 = RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.2493\n",
      "model2単体のMSE:\n",
      "0.4856\n",
      "model3単体のMSE:\n",
      "0.3472\n",
      "stackingを行った結果のMSE:\n",
      "0.2702\n"
     ]
    }
   ],
   "source": [
    "model3_stacking(X,y,\n",
    "                model1=LR,\n",
    "                model2 = CLF,\n",
    "                model3 = LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.4856\n",
      "model2単体のMSE:\n",
      "0.2704\n",
      "model3単体のMSE:\n",
      "0.1597\n",
      "stackingを行った結果のMSE:\n",
      "0.1719\n"
     ]
    }
   ],
   "source": [
    "model3_stacking(X,y,\n",
    "                model1=CLF,\n",
    "                model2 = TREE,\n",
    "                model3 = RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.4856\n",
      "model2単体のMSE:\n",
      "0.2751\n",
      "model3単体のMSE:\n",
      "0.3472\n",
      "stackingを行った結果のMSE:\n",
      "0.2320\n"
     ]
    }
   ],
   "source": [
    "model3_stacking(X,y,\n",
    "                model1=CLF,\n",
    "                model2 = TREE,\n",
    "                model3 = LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1単体のMSE:\n",
      "0.2737\n",
      "model2単体のMSE:\n",
      "0.1628\n",
      "model3単体のMSE:\n",
      "0.3472\n",
      "stackingを行った結果のMSE:\n",
      "0.1665\n"
     ]
    }
   ],
   "source": [
    "model3_stacking(X,y,\n",
    "                model1=TREE,\n",
    "                model2 = RFR,\n",
    "                model3 = LGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタッキングモデルによるMSEが、各モデルよりも性能が良かったのは\n",
    "<br>「CLF・TREE・LGBM」の３つを使った時でMSEは0.232だった。\n",
    "\n",
    "しかし最も性能が良かったのはランダムフォレストを単体で使う場合で、MSEは平均しておおよそ0.16だった。\n",
    "\n",
    "➡︎ランダムフォレストはどのアンサンブルと比べてもほとんどの場合で高い精度示した。\n",
    "<br>➡︎自らアンサンブルのモデルを構築するのは、労力に対して精度があまり良くない（良いモデルを探すのが大変）。\n",
    "<br>➡︎単に精度を上げたいのであれば、とりあえずとしてランダムフォレストを使うのは有効？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

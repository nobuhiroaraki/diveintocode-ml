{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.ディープニューラルネットワークスクラッチ\n",
    "\n",
    "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。\n",
    "\n",
    "その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
    "\n",
    "\n",
    "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
    "\n",
    "\n",
    "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。\n",
    "\n",
    "\n",
    "層などのクラス化\n",
    "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
    "\n",
    "\n",
    "手を加える箇所\n",
    "\n",
    "\n",
    "* 層の数\n",
    "* 層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
    "* 活性化関数の種類\n",
    "* 重みやバイアスの初期化方法\n",
    "* 最適化手法\n",
    "\n",
    "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
    "\n",
    "\n",
    "実装方法は自由ですが、簡単な例を紹介します。\n",
    "\n",
    "サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
    "\n",
    "\n",
    "《サンプルコード1》\n",
    "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
    "\n",
    "\n",
    "``` python\n",
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "optimizer = SGD(self.lr)\n",
    "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation1 = Tanh()\n",
    "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation2 = Tanh()\n",
    "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation3 = Softmax()\n",
    "```\n",
    "\n",
    "《サンプルコード2》\n",
    "イテレーションごとのフォワード\n",
    "\n",
    "```python\n",
    "A1 = self.FC1.forward(X)\n",
    "Z1 = self.activation1.forward(A1)\n",
    "A2 = self.FC2.forward(Z1)\n",
    "Z2 = self.activation2.forward(A2)\n",
    "A3 = self.FC3.forward(Z2)\n",
    "Z3 = self.activation3.forward(A3)\n",
    "```\n",
    "\n",
    "《サンプルコード3》\n",
    "イテレーションごとのバックワード\n",
    "\n",
    "```python\n",
    "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "dZ2 = self.FC3.backward(dA3)\n",
    "dA2 = self.activation2.backward(dZ2)\n",
    "dZ1 = self.FC2.backward(dA2)\n",
    "dA1 = self.activation1.backward(dZ1)\n",
    "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "```\n",
    "\n",
    "# ===========================================================\n",
    "# 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。\n",
    "\n",
    "\n",
    "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。\n",
    "\n",
    "重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
    "\n",
    "\n",
    "なお、インスタンスも引数として渡すことができます。\n",
    "\n",
    "そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。\n",
    "\n",
    "渡すインスタンスを変えれば、初期化方法が変えられます。\n",
    "\n",
    "\n",
    "また、引数として自身のインスタンスselfを渡すこともできます。\n",
    "\n",
    "これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。\n",
    "\n",
    "更新に必要な値は複数ありますが、全て全結合層が持つインスタンス変数にすることができます。\n",
    "\n",
    "初期化方法と最適化手法のクラスについては後述します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。\n",
    "\n",
    "\n",
    "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。\n",
    "\n",
    "以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
    "\n",
    "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。\n",
    "\n",
    "\n",
    "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。\n",
    "\n",
    "バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
    "\n",
    "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。\n",
    "\n",
    "\n",
    "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 発展的要素\n",
    "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。\n",
    "\n",
    "# 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
    "\n",
    "\n",
    "ReLUは以下の数式です。\n",
    "$$\n",
    "% <![CDATA[\n",
    "f(x) = ReLU(x) = \\begin{cases}\n",
    "x  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "\n",
    "x : ある特徴量。スカラー\n",
    "\n",
    "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
    "\n",
    "numpy.maximum — NumPy v1.15 Manual\n",
    "\n",
    "\n",
    "一方、バックプロパゲーションのための xに関する f(x)の微分は以下のようになります。\n",
    "\n",
    "$$\n",
    "% <![CDATA[\n",
    "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
    "1  & \\text{if $x>0$,}\\\\\n",
    "0 & \\text{if $x\\leqq0$.}\n",
    "\\end{cases} %]]>\n",
    "$$\n",
    "\n",
    "数学的には微分可能ではないですが、 x=0のとき 0とすることで対応しています。\n",
    "\n",
    "\n",
    "フォワード時の x の正負により、勾配を逆伝播するかどうかが決まるということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。\n",
    "\n",
    "しかし、どのような値にすると良いかが知られています。\n",
    "\n",
    "シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
    "\n",
    "\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。\n",
    "\n",
    "\n",
    "Xavierの初期値\n",
    "Xavierの初期値における標準偏差 \\sigma は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\sqrt{n}}\n",
    "$$\n",
    "n : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks.\n",
    "\n",
    "\n",
    "Heの初期値\n",
    "Heの初期値における標準偏差 \\sigma は次の式で求められます。\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{2}{n}}\n",
    "$$\n",
    "n : 前の層のノード数\n",
    "\n",
    "\n",
    "《論文》\n",
    "\n",
    "\n",
    "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
    "\n",
    "\n",
    "まず、これまで使ってきたSGDを確認します。\n",
    "\n",
    "$$\n",
    "W_i^{\\prime} = W_i - \\alpha E(\\frac{\\partial L}{\\partial W_i}) \\\\ B_i^{\\prime} = B_i - \\alpha E(\\frac{\\partial L}{\\partial B_i})\n",
    "$$\n",
    "$$\n",
    "\\alpha : 学習率（層ごとに変えることも可能だが、基本的には全て同じとする）\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_i} : W_i に関する損失 L の勾配\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B_i} : B_i に関する損失 L の勾配\n",
    "$$\n",
    "\n",
    "E() : ミニバッチ方向にベクトルの平均を計算\n",
    "\n",
    "\n",
    "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
    "\n",
    "\n",
    "更新された分だけその重みに対する学習率を徐々に下げていきます。\n",
    "\n",
    "イテレーションごとの勾配の二乗和 H を保存しておき、その分だけ学習率を小さくします。\n",
    "\n",
    "\n",
    "学習率は重み一つひとつに対して異なることになります。\n",
    "\n",
    "$$\n",
    "H_i^{\\prime} = H_i+E(\\frac{\\partial L}{\\partial W_i})×E(\\frac{\\partial L}{\\partial W_i})\\\\ W_i^{\\prime} = W_i - \\alpha \\frac{1}{\\sqrt{H_i^{\\prime} }} E(\\frac{\\partial L}{\\partial W_i}) \\\\\n",
    "$$\n",
    "H_i : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "H_i^{\\prime} : 更新した H_i\n",
    "$$\n",
    "《論文》\n",
    "\n",
    "\n",
    "Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===========================================================\n",
    "# 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(palette=\"bright\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "#前処理\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "#分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "#目的変数をワンホット化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作成したクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================ミニバッチ===================================================\n",
    "#ミニバッチを取得するクラス\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "#===========================================全結合層===================================================\n",
    "#（問1）\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        \n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        \n",
    "        #初期化手法\n",
    "        self.initializer = initializer\n",
    "        \n",
    "        #最適化手法\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # 指定したinitializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = self.initializer.W(self.n_nodes1,self.n_nodes2)\n",
    "        self.B = self.initializer.B(self.n_nodes2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        #フォワード時の入力Xをインスタンス変数として保持\n",
    "        self.X = X\n",
    "        \n",
    "        #順伝播\n",
    "        self.A = self.X@self.W + self.B\n",
    "        \n",
    "        return self.A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 逆伝播\n",
    "        self.dB = np.sum(dA,axis=0)#dA・・・逆伝播する際に活性化関数に通して出力された値\n",
    "        self.dW = self.X.T@dA\n",
    "        self.dZ = dA@self.W.T\n",
    "        \n",
    "        #インスタンスの重みとバイアス自身を指定した最適手法により求めた値で更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        return self.dZ\n",
    "\n",
    "#===========================================初期化手法===================================================\n",
    "#（問２）\n",
    "#ガウス分布によるシンプルな初期化手法\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        W = self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :バイアスの初期値\n",
    "        \"\"\"\n",
    "        B = self.sigma*np.random.randn(1,n_nodes2)\n",
    "        return B\n",
    "\n",
    "#(問６)\n",
    "# Xavierの初期値による初期化手法\n",
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        #Xavierの初期値における標準偏差 sigma \n",
    "        self.sigma = 1/np.sqrt(n_nodes1)\n",
    "        \n",
    "        #上記sigmaを用いて重みの初期値を求める\n",
    "        W = self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :バイアスの初期値\n",
    "        \"\"\"\n",
    "        #上記sigmaを用いてバイアスの初期値を求める\n",
    "        B = self.sigma*np.random.randn(1,n_nodes2)\n",
    "        \n",
    "        return B\n",
    "\n",
    "\n",
    "#（問６）\n",
    "# Heの初期値による初期化手法\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "    \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :　重みの初期値\n",
    "        \"\"\"\n",
    "        #Heの初期値における標準偏差 sigma \n",
    "        self.sigma = np.sqrt(2/n_nodes1)\n",
    "        \n",
    "        #上記sigmaを用いて重みの初期値を求める\n",
    "        W = self.sigma*np.random.randn(n_nodes1,n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :バイアスの初期値\n",
    "        \"\"\"\n",
    "        #上記sigmaを用いてバイアスの初期値を求める\n",
    "        B = self.sigma*np.random.randn(1,n_nodes2)\n",
    "        \n",
    "        return B\n",
    "\n",
    "#===========================================活性化関数===================================================\n",
    "#(問４)  \n",
    "#sigmoid関数を活性化関数として使用\n",
    "class Sigmoid:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 各層に入力される値(batch_size, n_nodes_1)\n",
    "        dZ : バックワード時に前に流す勾配(batch_size, n_nodes1)\n",
    "\n",
    "       Returns\n",
    "        -------\n",
    "        Z : 各層からの出力される値(batch_size, n_nodes1)\n",
    "        dA : バックワード時に後ろから流れてきた勾配(batch_size, n_nodes2)\n",
    "        \"\"\"\n",
    "        def forward(self,A):\n",
    "            Z = 1/(1+np.exp(-A))\n",
    "            \n",
    "            return Z\n",
    "        \n",
    "        def backward(self,A,dZ):\n",
    "            dA = dZ*(1- self.forward(A)**2)\n",
    "            \n",
    "            return dA    \n",
    "    \n",
    "    \n",
    "#（問４）   \n",
    "#ハイパボリックタンジェント関数を活性化関数として使用\n",
    "class Tanh:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 各層に入力される値(batch_size, n_nodes_1)\n",
    "        dZ : バックワード時に前に流す勾配(batch_size, n_nodes1)\n",
    "\n",
    "       Returns\n",
    "        -------\n",
    "        Z : 各層からの出力される値(batch_size, n_nodes1)\n",
    "        dA : バックワード時に後ろから流れてきた勾配(batch_size, n_nodes2)\n",
    "        \"\"\"\n",
    "        def forward(self,A):\n",
    "            Z = np.tanh(A)\n",
    "            \n",
    "            return Z\n",
    "         \n",
    "        def backward(self,A,dZ):\n",
    "            dA = dZ*(1 - np.tanh(A)**2)\n",
    "\n",
    "            return dA\n",
    "    \n",
    "    \n",
    "    \n",
    "#(問５)\n",
    "#ReLU関数を活性化関数として使用\n",
    "class ReLU:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 各層に入力される値(batch_size, n_nodes_1)\n",
    "        dZ : バックワード時に前に流す勾配(batch_size, n_nodes1)\n",
    "\n",
    "       Returns\n",
    "        -------\n",
    "        Z : 各層からの出力される値(batch_size, n_nodes1)\n",
    "        dA : バックワード時に後ろから流れてきた勾配(batch_size, n_nodes2)\n",
    "        \"\"\"\n",
    "        def forward(self,A):\n",
    "            #0か入力か大きい方が出力される\n",
    "            self.Z = np.maximum(0,A)\n",
    "            \n",
    "            \n",
    "            return self.Z\n",
    "         \n",
    "        def backward(self,A,dZ):\n",
    "            #入力が０以上なら１に勾配をかける、0以下なら０を出力\n",
    "\n",
    "            dA = dZ*np.where(A>0,1,0)\n",
    "            \n",
    "            return dA    \n",
    "\n",
    "\n",
    "#（問４）\n",
    "#ソフトマックス関数を活性化関数として使用\n",
    "class Softmax:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 各層に入力される値(batch_size, n_nodes_1)\n",
    "        dZ : バックワード時に前に流す勾配(batch_size, n_nodes1)\n",
    "\n",
    "       Returns\n",
    "        -------\n",
    "        Z : 各層からの出力される値(batch_size, n_nodes1)\n",
    "        dA : バックワード時に後ろから流れてきた勾配(batch_size, n_nodes2)\n",
    "        \"\"\"\n",
    "        def forward(self,A):\n",
    "            Z = np.exp(A)/np.sum(np.exp(A),axis=1).reshape(-1,1)\n",
    "            \n",
    "            return Z\n",
    "         \n",
    "        def backward(self,A,y):\n",
    "            dA = self.forward(A) - y\n",
    "            #back時は交差エントロピー誤差も計算\n",
    "            L = - np.sum(y * np.log(self.forward(A))) / len(y)\n",
    "            \n",
    "            return dA,L        \n",
    "        \n",
    "#===========================================最適化手法===================================================\n",
    "#（問３）\n",
    "#最適化手法（SGD）\n",
    "class SGD():\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "       \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #パラメータを誤差逆伝播で算出したものに学習率をかけて更新\n",
    "        layer.W -= self.lr*layer.dW/20#バッチサイズで割る\n",
    "        layer.B -= self.lr*layer.dB/20\n",
    "        \n",
    "        #パラメータを更新したインスタンスを返す\n",
    "        return layer\n",
    "\n",
    "#（問７）\n",
    "#最適化手法（AdaGrad）\n",
    "class AdaGrad():\n",
    "    \"\"\"\n",
    "    AdaGradを用いた最適化\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        #HW,HBの初期値\n",
    "        layer.HW = 0\n",
    "        layer.HB = 0\n",
    "        \n",
    "        #パラメータを更新\n",
    "        layer.HW += layer.dW*layer.dW\n",
    "        layer.HB += layer.dB*layer.dB\n",
    "        layer.W -= self.lr*layer.dW/(np.sqrt(layer.HW)+ 1e-7)/20#1e-7・・・np.sqrt(layer.HW)が０になることが考えられるため、#結果に影響しない程度の小さい数を加える\n",
    "        layer.B-= self.lr*layer.dB/(np.sqrt(layer.HB) + 1e-7)/20\n",
    "        \n",
    "        #パラメータを更新したインスタンスを返す\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================================\n",
    "# ４層のScratchDeepNeuralNetrowkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4層のDNN\n",
    "class ScratchDeepNeuralNetrowkClassifier4():\n",
    "    \"\"\"\n",
    "    4層のディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,n_features=784,n_nodes1=400,n_nodes2=200,n_nodes3=100,n_output=10,sigma=0.1,lr=0.01,batch_size=20,epoch=10,initializer=SimpleInitializer,optimizer=SGD,activeter=ReLU(),verbose=False):\n",
    "\n",
    "        self.n_features = n_features#特徴力の数\n",
    "        self.n_nodes1 = n_nodes1#１層目のノード数\n",
    "        self.n_nodes2 = n_nodes2#2層目のノード数\n",
    "        self.n_nodes3 = n_nodes3#3層目のノード数\n",
    "        self.n_output = n_output#4層目のノード数(最終的に出力する数)\n",
    "        self.sigma = sigma#ガウス分布の標準偏差\n",
    "        self.lr = lr#学習率\n",
    "        self.batch_size = batch_size#バッチサイズ\n",
    "        self.epoch = epoch#エポック数\n",
    "        self.initializer = initializer#初期化手法\n",
    "        self.optimizer = optimizer#最適化手法\n",
    "        self.activeter = activeter#活性化関数\n",
    "        self.verbose = verbose#学習経過を出力するか\n",
    "        \n",
    "        #学習曲線を描画するためにクロスエントロピー誤差を保存するリスト\n",
    "        self.loss_lst = []\n",
    "        self.val_loss_lst = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,X,y,X_val=None,y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        #各層ごとにFCクラスを呼び出しインスタンスを作成することで、指定した初期化手法で各層における重みとバイアスを初期化\n",
    "        self.FC1 = FC(self.n_features,self.n_nodes1,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC2 = FC(self.n_nodes1,self.n_nodes2,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC3 = FC(self.n_nodes2,self.n_nodes3,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC4 = FC(self.n_nodes3,self.n_output,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        \n",
    "        #指定したエポック分だけ繰り返す\n",
    "        for epoch in range(self.epoch):\n",
    "            #サンプルをミニバッチ処理して、ミニバッチずつ学習させる\n",
    "            get_mini_batch = GetMiniBatch(X, y, self.batch_size,seed=epoch) \n",
    "            #n回目のエポックにおけるミニバッチを取得、各ミニバッチで一連の流れを繰り返す\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                #miniX_train(20, 784)\n",
    "                #miniy_train(20, 10)\n",
    "                \n",
    "                #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "                self.Z4 = self.forward(mini_X_train)\n",
    "                #A1(20, 400)\n",
    "                #Z1(20, 400)\n",
    "                #A2(20, 200)\n",
    "                #Z2(20, 200)\n",
    "                #A3(20, 100)\n",
    "                #Z3(20, 100)\n",
    "                #A4(20, 10)\n",
    "                #Z4(20, 10)\n",
    "\n",
    "                #誤差逆伝播(通ってきた道を活性化関数（backward版）通しながら引き返して最適な重みとバイアスを探す)\n",
    "                #４層目への入力と、交差エントロピー誤差を算出する\n",
    "                self.dA4,self.loss = self.softmax.backward(self.Z4,mini_y_train)\n",
    "                #self.dA4(20, 10)\n",
    "                #3層目からの出力\n",
    "                self.dZ3 = self.FC4.backward(self.dA4)\n",
    "                #self.dZ3(20, 100)\n",
    "                #3層目への入力\n",
    "                self.dA3 = self.activeter.backward(self.A3,self.dZ3)\n",
    "                #self.dA3(20, 100)  \n",
    "                #2層目からの出力\n",
    "                self.dZ2 = self.FC3.backward(self.dA3)\n",
    "                #self.dZ2(20, 200)\n",
    "                #2層目への入力\n",
    "                self.dA2 = self.activeter.backward(self.A2,self.dZ2)\n",
    "                #self.dA2(20, 200)\n",
    "                #1層目からの出力\n",
    "                self.dZ1 = self.FC2.backward(self.dA2)\n",
    "                #self.dZ1(20, 400)\n",
    "                #1層目への入力\n",
    "                self.dA1 = self.activeter.backward(self.A1,self.dZ1)\n",
    "                #self.dA1(20, 400)\n",
    "                self.dZ0 = self.FC1.backward(self.dA1)\n",
    "                \n",
    "            #求めたパラメータで再度通る\n",
    "            #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "            self.Z4 = self.forward(mini_X_train)\n",
    "            \n",
    "            #エポックごとに交差エントロピー誤差を算出しリストに入れる\n",
    "            #L = self.softmax.backward(self.Z4,mini_y_train)[1]\n",
    "            #self.loss_lst.append(L)\n",
    "            \n",
    "            #valデータが入力された場合はvalデータも順伝播＆誤差保存\n",
    "            if X_val is not None and y_val is not None:\n",
    "                #順伝播(X_valを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "                self.Z4 = self.forward(X_val)\n",
    "\n",
    "                #エポックごとに交差エントロピー誤差を算出しリストに入れる\n",
    "               #val_L = self.softmax.backward(self.Z4,y_val)[1]\n",
    "                #self.val_loss_lst.append(val_L)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は各エポックごとの損失の推移を出力する\n",
    "                print(\"Epoch{}\".format(epoch))\n",
    "                print(\"loss:{:.10f}/val_loss:{:.10f}\".format(L,val_L))\n",
    "     \n",
    "    \n",
    "    def forward(self,X):\n",
    "        #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "        #１層目\n",
    "        self.A1 = self.FC1.forward(X)\n",
    "        self.Z1 = self.activeter.forward(self.A1)\n",
    "        #２層目\n",
    "        self.A2 = self.FC2.forward(self.Z1)\n",
    "        self.Z2 = self.activeter.forward(self.A2)\n",
    "        #3層目\n",
    "        self.A3 = self.FC3.forward(self.Z2)\n",
    "        self.Z3 = self.activeter.forward(self.A3)\n",
    "        #4層目\n",
    "        self.A4 = self.FC4.forward(self.Z3)\n",
    "        #最後の層だけソフトマックス関数を通す\n",
    "        self.softmax = Softmax()\n",
    "        self.Z4 = self.softmax.forward(self.A4)\n",
    "        \n",
    "        return self.Z4\n",
    "                \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        推定結果\n",
    "        \"\"\"\n",
    "        self.Z4 = self.forward(X)\n",
    "        true_pred = np.argmax(self.Z4,axis=1)#softmax関数で返された確率の中で１番大きい要素のインデックス（＝予測した数字）を取得する\n",
    "        return true_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================================\n",
    "# 6層のScratchDeepNeuralNetrowkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6層のDNN\n",
    "class ScratchDeepNeuralNetrowkClassifier6():\n",
    "    \"\"\"\n",
    "    4層のディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self,n_features=784,n_nodes1=1600,n_nodes2=800,n_nodes3=400,n_nodes4=200,n_nodes5=100,n_output=10,sigma=0.1,lr=0.01,batch_size=20,epoch=10,initializer=SimpleInitializer,optimizer=SGD,activeter=ReLU(),verbose=False):\n",
    "\n",
    "        self.n_features = n_features#特徴力の数\n",
    "        self.n_nodes1 = n_nodes1#１層目のノード数\n",
    "        self.n_nodes2 = n_nodes2#2層目のノード数\n",
    "        self.n_nodes3 = n_nodes3#3層目のノード数\n",
    "        self.n_nodes4 = n_nodes4#4層目のノード数\n",
    "        self.n_nodes5 = n_nodes5#5層目のノード数\n",
    "        self.n_output = n_output#6層目のノード数(最終的に出力する数)\n",
    "        self.sigma = sigma#ガウス分布の標準偏差\n",
    "        self.lr = lr#学習率\n",
    "        self.batch_size = batch_size#バッチサイズ\n",
    "        self.epoch = epoch#エポック数\n",
    "        self.initializer = initializer#初期化手法\n",
    "        self.optimizer = optimizer#最適化手法\n",
    "        self.activeter = activeter#活性化関数\n",
    "        self.verbose = verbose#学習経過を出力するか\n",
    "        \n",
    "        #学習曲線を描画するためにクロスエントロピー誤差を保存するリスト\n",
    "        self.loss_lst = []\n",
    "        self.val_loss_lst = []\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit(self,X,y,X_val=None,y_val=None):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        #各層ごとにFCクラスを呼び出しインスタンスを作成することで、指定した初期化手法で各層における重みとバイアスを初期化\n",
    "        self.FC1 = FC(self.n_features,self.n_nodes1,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC2 = FC(self.n_nodes1,self.n_nodes2,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC3 = FC(self.n_nodes2,self.n_nodes3,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC4 = FC(self.n_nodes3,self.n_nodes4,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC5 = FC(self.n_nodes4,self.n_nodes5,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        self.FC6 = FC(self.n_nodes5,self.n_output,self.initializer(self.sigma),self.optimizer(self.lr))\n",
    "        \n",
    "        #指定したエポック分だけ繰り返す\n",
    "        for epoch in range(self.epoch):\n",
    "            #サンプルをミニバッチ処理して、ミニバッチずつ学習させる\n",
    "            get_mini_batch = GetMiniBatch(X, y, self.batch_size,seed=epoch) \n",
    "            #n回目のエポックにおけるミニバッチを取得、各ミニバッチで一連の流れを繰り返す\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                \n",
    "                #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "                self.Z6 = self.forward(mini_X_train)\n",
    "\n",
    "                #誤差逆伝播(通ってきた道を活性化関数（backward版）通しながら引き返して最適な重みとバイアスを探す)\n",
    "                #6層目への入力と、交差エントロピー誤差を算出する\n",
    "                self.dA6,self.loss = self.softmax.backward(self.Z6,mini_y_train)\n",
    "                #5層目からの出力\n",
    "                self.dZ5 = self.FC6.backward(self.dA6)  \n",
    "                #5層目への入力\n",
    "                self.dA5 = self.activeter.backward(self.A5,self.dZ5)\n",
    "                #4層目からの出力\n",
    "                self.dZ4 = self.FC5.backward(self.dA5)\n",
    "                #4層目への入力\n",
    "                self.dA4 = self.activeter.backward(self.A4,self.dZ4)\n",
    "                #3層目からの出力\n",
    "                self.dZ3 = self.FC4.backward(self.dA4)\n",
    "                #3層目への入力\n",
    "                self.dA3 = self.activeter.backward(self.A3,self.dZ3)\n",
    "                #2層目からの出力\n",
    "                self.dZ2 = self.FC3.backward(self.dA3)\n",
    "                #2層目への入力\n",
    "                self.dA2 = self.activeter.backward(self.A2,self.dZ2)                \n",
    "                #1層目からの出力\n",
    "                self.dZ1 = self.FC2.backward(self.dA2)                \n",
    "                #1層目への入力\n",
    "                self.dA1 = self.activeter.backward(self.A1,self.dZ1)\n",
    "                self.dZ0 = self.FC1.backward(self.dA1)\n",
    "                \n",
    "            #求めたパラメータで再度通る\n",
    "            #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "            self.Z6 = self.forward(mini_X_train)\n",
    "            \n",
    "            #エポックごとに交差エントロピー誤差を算出しリストに入れる\n",
    "            #L = self.softmax.backward(self.Z6,mini_y_train)[1]\n",
    "            #self.loss_lst.append(L)\n",
    "            \n",
    "            #valデータが入力された場合はvalデータも順伝播＆誤差保存\n",
    "            if X_val is not None and y_val is not None:\n",
    "                #順伝播(X_valを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "                self.Z6 = self.forward(X_val)\n",
    "\n",
    "                #エポックごとに交差エントロピー誤差を算出しリストに入れる\n",
    "                #val_L = self.softmax.backward(self.Z6,y_val)[1]\n",
    "                #self.val_loss_lst.append(val_L)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は各エポックごとの損失の推移を出力する\n",
    "                print(\"Epoch{}\".format(epoch))\n",
    "                print(\"loss:{:.10f}/val_loss:{:.10f}\".format(L,val_L))\n",
    "     \n",
    "    \n",
    "    def forward(self,X):\n",
    "        #順伝播(mini_X_trainを入力値として受け取って、全ての層を活性化関数を適用しながら通る)\n",
    "        #１層目\n",
    "        self.A1 = self.FC1.forward(X)\n",
    "        self.Z1 = self.activeter.forward(self.A1)\n",
    "        #２層目\n",
    "        self.A2 = self.FC2.forward(self.Z1)\n",
    "        self.Z2 = self.activeter.forward(self.A2)\n",
    "        #3層目\n",
    "        self.A3 = self.FC3.forward(self.Z2)\n",
    "        self.Z3 = self.activeter.forward(self.A3)\n",
    "        #4層目\n",
    "        self.A4 = self.FC4.forward(self.Z3)\n",
    "        self.Z4= self.activeter.forward(self.A4)\n",
    "        #5層目\n",
    "        self.A5 = self.FC5.forward(self.Z4)\n",
    "        self.Z5 = self.activeter.forward(self.A5)\n",
    "        #6層目\n",
    "        self.A6 = self.FC6.forward(self.Z5)\n",
    "        #最後の層だけソフトマックス関数を通す\n",
    "        self.softmax = Softmax()\n",
    "        self.Z6 = self.softmax.forward(self.A6)\n",
    "        \n",
    "        return self.Z6\n",
    "                \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        推定結果\n",
    "        \"\"\"\n",
    "        self.Z6 = self.forward(X)\n",
    "        true_pred = np.argmax(self.Z6,axis=1)#softmax関数で返された確率の中で１番大きい要素のインデックス（＝予測した数字）を取得する\n",
    "        return true_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "====================================================================================\n",
    "# 【問題9】学習と推定\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ４層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SimpleInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nobu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:284: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31092\n",
      "[Tanh]\n",
      "0.08992\n",
      "[ReLU]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nobu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:364: RuntimeWarning: overflow encountered in exp\n",
      "/Users/nobu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:364: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/nobu/.pyenv/versions/anaconda3-5.1.0/lib/python3.6/site-packages/ipykernel_launcher.py:344: RuntimeWarning: invalid value encountered in greater\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09408\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.14125\n",
      "[Tanh]\n",
      "0.09458\n",
      "[ReLU]\n",
      "0.09408\n",
      "=====HeInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.43958\n",
      "[ReLU]\n",
      "0.15892\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.62875\n",
      "[ReLU]\n",
      "0.24\n",
      "=====XavierInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.20975\n",
      "[ReLU]\n",
      "0.13542\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.61067\n",
      "[ReLU]\n",
      "0.18283\n"
     ]
    }
   ],
   "source": [
    "initializer_lst = [SimpleInitializer,HeInitializer,XavierInitializer]\n",
    "initializer_name = [\"=====SimpleInitializer=====\",\"=====HeInitializer=====\",\"=====XavierInitializer=====\"]\n",
    "optimizer_lst = [SGD,AdaGrad]\n",
    "optimizer_name = [\"--------SGD--------\",\"--------AdaGrad--------\"]\n",
    "activeter_lst = [Sigmoid,Tanh,ReLU]\n",
    "activeter_name =  [\"[Sigmoid]\",\"[Tanh]\",\"[ReLU]\"]\n",
    "\n",
    "\n",
    "#各手法全ての組み合わせでインスタンスを作成・学習・推定・acc算出\n",
    "for initial,initial_name in zip (initializer_lst,initializer_name):\n",
    "    print(initial_name)\n",
    "    for optim,optim_name in zip(optimizer_lst,optimizer_name):\n",
    "        print(optim_name)\n",
    "        for active,active_name in zip(activeter_lst,activeter_name):\n",
    "            print(active_name)\n",
    "            SDNC4 = ScratchDeepNeuralNetrowkClassifier4(\n",
    "    n_features=784,\n",
    "    n_nodes1=800,\n",
    "    n_nodes2=400,\n",
    "    n_nodes3=200,\n",
    "    n_output=10,\n",
    "    sigma=1,\n",
    "    lr=0.004,\n",
    "    batch_size=20,\n",
    "    epoch=20,\n",
    "    initializer=initial,\n",
    "    optimizer=optim,\n",
    "    activeter=active(),\n",
    "    verbose=False\n",
    ")\n",
    "            #学習\n",
    "            SDNC4.fit(X_train[:100],y_train_one_hot[:100],X_val[:100],y_test_one_hot[:100])\n",
    "\n",
    "\n",
    "            #推定\n",
    "            y_pred = SDNC4.predict(X_val)\n",
    "\n",
    "\n",
    "            # accuracy_scoreを算出 \n",
    "            acc = accuracy_score(y_val,y_pred)\n",
    "            print(round(acc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====SimpleInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.09617\n",
      "[ReLU]\n",
      "0.09617\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.20283\n",
      "[ReLU]\n",
      "0.09408\n",
      "=====HeInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n",
      "0.09967\n",
      "[Tanh]\n",
      "0.3665\n",
      "[ReLU]\n",
      "0.15042\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.10108\n",
      "[Tanh]\n",
      "0.6185\n",
      "[ReLU]\n",
      "0.28625\n",
      "=====XavierInitializer=====\n",
      "--------SGD--------\n",
      "[Sigmoid]\n",
      "0.09967\n",
      "[Tanh]\n",
      "0.27067\n",
      "[ReLU]\n",
      "0.10142\n",
      "--------AdaGrad--------\n",
      "[Sigmoid]\n",
      "0.09408\n",
      "[Tanh]\n",
      "0.58042\n",
      "[ReLU]\n",
      "0.22325\n"
     ]
    }
   ],
   "source": [
    "initializer_lst = [SimpleInitializer,HeInitializer,XavierInitializer]\n",
    "initializer_name = [\"=====SimpleInitializer=====\",\"=====HeInitializer=====\",\"=====XavierInitializer=====\"]\n",
    "optimizer_lst = [SGD,AdaGrad]\n",
    "optimizer_name = [\"--------SGD--------\",\"--------AdaGrad--------\"]\n",
    "activeter_lst = [Sigmoid,Tanh,ReLU]\n",
    "activeter_name =  [\"[Sigmoid]\",\"[Tanh]\",\"[ReLU]\"]\n",
    "\n",
    "\n",
    "#各手法全ての組み合わせでインスタンスを作成・学習・推定・acc算出\n",
    "for initial,initial_name in zip (initializer_lst,initializer_name):\n",
    "    print(initial_name)\n",
    "    for optim,optim_name in zip(optimizer_lst,optimizer_name):\n",
    "        print(optim_name)\n",
    "        for active,active_name in zip(activeter_lst,activeter_name):\n",
    "            print(active_name)\n",
    "            SDNC6 = ScratchDeepNeuralNetrowkClassifier6(\n",
    "    n_features=784,\n",
    "    n_nodes1=800,\n",
    "    n_nodes2=400,\n",
    "    n_nodes3=200,\n",
    "    n_nodes4=100,\n",
    "    n_nodes5=100,\n",
    "    n_output=10,\n",
    "    sigma=0.01,\n",
    "    lr=0.004,\n",
    "    batch_size=20,\n",
    "    epoch=10,\n",
    "    initializer=initial,\n",
    "    optimizer=optim,\n",
    "    activeter=active(),\n",
    "    verbose=False\n",
    ")\n",
    "            #学習\n",
    "            SDNC6.fit(X_train[:100],y_train_one_hot[:100],X_val[:50],y_test_one_hot[:50])\n",
    "\n",
    "\n",
    "            #推定\n",
    "            y_pred = SDNC6.predict(X_val)\n",
    "\n",
    "\n",
    "            # accuracy_scoreを算出 \n",
    "            acc = accuracy_score(y_val,y_pred)\n",
    "            print(round(acc,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 考察\n",
    "\n",
    "➡︎４層と６層を比べる以前に、各手法の組み合わせによって最適なハイパーパラメータが違いすぎた。\n",
    "\n",
    "今回手法の組み合わせによる優劣を比べるために、全ての組み合わせで同一のパラメータを使った。\n",
    "\n",
    "しかしそれぞれの手法の組み合わせは、わずかなパラメータの差で結果に大きく影響があり、単純に優劣を比べることができなかった。（パラメータによって上手く行く組み合わせと、全て０と判定してしまうような上手くいかない組み合わせがあった。（例：sigmoidを使った時にaccが高くなるようなパラメータではReLUが低くなったり、、、)）\n",
    "\n",
    "そのため、NNの精度はわずかなパラメータの差が大きく影響を及ぼすこと、手法の組み合わせによって最適なパラメータが違うことがわかった。\n",
    "\n",
    "こうしたことからNNのモデルを構築する際には、\n",
    "適当に手法を組み合わせるのではなく、各手法の特徴を理解した上でそれぞれを組み合わせる必要があると考えられる。\n",
    "\n",
    "また、各手法の特徴を理解し、手法の組み合わせを何パターンかに絞り込んだ上で、\n",
    "最適な層数やノード数、パラメータを探索することができればより効率的に精度の高いモデルを構築できると感じた。\n",
    "\n",
    "そのためにはやはりアルゴリズムの理解が重要なのだなと改めて実感した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

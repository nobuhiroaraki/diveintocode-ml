{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.論文読解\n",
    "\n",
    "以下の論文を読み問題に答えてください。CNNを使った物体検出（Object Detection）の代表的な研究です。\n",
    "\n",
    "\n",
    "[8]Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: Advances in neural information processing systems. (2015) 91–99\n",
    "\n",
    "\n",
    "https://arxiv.org/pdf/1506.01497.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題\n",
    "それぞれについてJupyter Notebookにマークダウン形式で記述してください。\n",
    "\n",
    "\n",
    "(1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "\n",
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n",
    "\n",
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n",
    "\n",
    "(4) RPNとは何か。\n",
    "\n",
    "\n",
    "(5) RoIプーリングとは何か。\n",
    "\n",
    "\n",
    "(6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "\n",
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。\n",
    "\n",
    "\n",
    "(8) （アドバンス課題）Faster R-CNNよりも新しい物体検出の論文では、Faster R-CNNがどう引用されているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) 物体検出の分野にはどういった手法が存在したか。\n",
    "\n",
    "* Exhaustive\n",
    "\n",
    "➡︎基礎となる画像構造を使用して、オブジェクトの場所を生成する。\n",
    "\n",
    "We use the underlying image structure to generate object locations. *[4]p2\n",
    "\n",
    "* Segmentation\n",
    "\n",
    "➡︎複数の前景/背景セグメンテーションを生成し、前景セグメントが完全なオブジェクトである可能性を予測する方法を学び、これを使用してセグメントをランク付けする。\n",
    "\n",
    "generate multiple foreground/background segmentations, learn to predict the likelihood that a foreground segment is a complete object, and use this to rank the segments. *[4]p2\n",
    "\n",
    "* Selective Search\n",
    "\n",
    "➡︎ボトムアップセグメンテーションに着想を得て、画像の構造を利用してオブジェクトの場所を生成する\n",
    "\n",
    " Inspired by bottom-up segmentation, we aim to exploit the structure of the image to generate object locations.\n",
    " *[4]p3\n",
    "\n",
    "*上記3つは本論の引用文献に記されていた[4]の論文にて紹介されていた。\n",
    "\n",
    "[4]J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders, “Selective search for object recognition,”International Journal of Computer Vision (IJCV), 2013.\n",
    "https://staff.fnwi.uva.nl/th.gevers/pub/GeversIJCV2013.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Fasterとあるが、どういった仕組みで高速化したのか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 深い畳み込みネットワークを使用してオブジェクトの提案を効率的に分類する。\n",
    " \n",
    " Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. *[2]p1\n",
    "\n",
    "引用文献[２]より引用しました\n",
    "[2]R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.\n",
    "\n",
    "https://arxiv.org/pdf/1504.08083.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Stage : クラス固有の検出パイプライン\n",
    "\n",
    "Two-Stage : クラスにとらわれない提案とクラス固有の検出で構成される。領域ごとの機能が、領域の機能をより忠実にカバーする提案ボックスから適応的にプールされ、より正確な検出につながる\n",
    "\n",
    "\n",
    "The OverFeat paper [9] proposes a detection method that uses regressors and classifiers on sliding windows over convolutional feature maps. *p１０\n",
    "\n",
    "\n",
    " In the second stage of our cascade, the region-wise features are adaptively pooled [1], [2] from proposal boxes that more faith- fully cover the features of the regions. We believe these features lead to more accurate detections. *p10\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) RPNとは何か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡︎検出ネットワークとフルイメージのたたみ込み機能を共有するリージョン提案ネットワーク\n",
    "\n",
    "we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.*p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) RoIプーリングとは何か。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoIプーリングレイヤーRoIプーリングレイヤーは、最大プーリングを使用して、有効な関心領域内のフィーチャをH×Wの固定空間範囲（たとえば、7×7）の小さなフィーチャマップに変換する\n",
    "\n",
    "The RoI pooling layer The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are independent of any particular RoI. *[2]p2\n",
    "\n",
    "[2]R. Girshick, “Fast R-CNN,” in IEEE International Conference on Computer Vision (ICCV), 2015.\n",
    "https://arxiv.org/pdf/1504.08083.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# (6) Anchorのサイズはどうするのが適切か。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "アンカーのデザインには二つ手法があり、\n",
    "\n",
    "1つはDPM およびCNNベースの方法の画像/機能ピラミッドに基づいたもの。\n",
    "\n",
    "2つ目は機能マップで複数の縮尺（および/またはアスペクト比）のスライディングウィンドウを使用すること\n",
    "\n",
    "1つ目は時間がかかりすぎるため、通常2つ目が採用される\n",
    "\n",
    "\n",
    "Our design of anchors presents a novel scheme for addressing multiple scales (and aspect ratios). As shown in Figure 1, there have been two popular ways for multi-scale predictions. The first way is based on image/feature pyramids, e.g., in DPM [8] and CNN- based methods [9], [1], [2]. The images are resized at multiple scales, and feature maps (HOG [8] or deep convolutional features [9], [1], [2]) are computed for each scale (Figure 1(a)). This way is often useful but is time-consuming. The second way is to use sliding windows of multiple scales (and/or aspect ratios) on the feature maps. For example, in DPM [8], models of different aspect ratios are trained separately using different filter sizes (such as 5×7 and 7×5). p4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセット：PASCAL VOC 2007\n",
    "\n",
    "実行時間は短くなり、領域提案の品質も改善するため全体的なオブジェクト検出精度も向上した。\n",
    "\n",
    "\n",
    "We have presented RPNs for efficient and accurate region proposal generation. By sharing convolutional features with the down-stream detection network, the region proposal step is nearly cost-free. Our method enables a unified, deep-learning-based object detec- tion system to run at near real-time frame rates. The learned RPN also improves region proposal quality and thus the overall object detection accuracy.*p12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
